{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1. Write a Python function compute_slope_estimator, which takes in two input variables, x and y. The variables x and y should be 1-dimensional NumPy arrays that have the same length n. The function should return the optimal value of the slope from Equation 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_slope_estimator(x,y):\n",
    "    n = x.shape[0]\n",
    "    x_mean = np.sum(x,axis = 0) / n\n",
    "    y_mean = np.sum(y,axis = 0) / n\n",
    "    sum_xy = np.sum(x*y, axis = 0)\n",
    "    sum_xx = np.sum(x*x, axis = 0)\n",
    "    a = (sum_xy - n * x_mean * y_mean) / (sum_xx - n*x_mean*x_mean)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2. Write a Python function compute_intercept_estimator, which takes in two input variables, x and y. The variables x and y should be 1-dimensional NumPy arrays that have the same length n. The function should return the optimal value of the intercept from Equation 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intercept_estimator(x,y):\n",
    "    n = x.shape[0]\n",
    "    x_mean = np.sum(x,axis = 0) / n\n",
    "    y_mean = np.sum(y,axis = 0) / n\n",
    "    sum_xy = np.sum(x*y, axis = 0)\n",
    "    sum_xx = np.sum(x*x, axis = 0)\n",
    "    a = (sum_xy - n * x_mean * y_mean) / (sum_xx - n*x_mean*x_mean)\n",
    "    b = y_mean - a * x_mean\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3. Write a function train_model, which takes in two 1-dimensional NumPy ar-\n",
    "rays of the same length, x and y. It should use compute_slope_estimator and compute_intercept_estimator, and return a tuple of values: the optimal value of the slope and the optimal value of the\n",
    "intercept.\n",
    "The elements in the array y can be considered our training set: we use them to estimate the optimal values of the slope and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x,training_set):\n",
    "    return (compute_slope_estimator(x,training_set), compute_intercept_estimator(x,training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4. Write a function sample_linear_model which takes four arguments: x_vals, a, b, and sd. The variable x_vals is a 1-dimensional NumPy array of length n. The function should return a NumPy array y of length n, where each element of yi has been sampled from: yi = a · xi + b + εi. Here εi should follow a normal distribution with mean equal to 0 and standard deviation equal to sd.\n",
    "This function describes the generative model that we believe our dataset was sampled from.\n",
    "(You should use NumPy’s built in functions for sampling from a normal distribution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_linear_model(x,slope,intercept,sd):\n",
    "    n = x.shape[0]\n",
    "    e = np.random.normal(0 , sd, n)\n",
    "    y = slope * x + intercept + e\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5. Write a function sample_datasets which takes five arguments (the first four the same as in the previous problem): x_vals, a, b, sd, and n. It should return a list of n sampled datasets, where each dataset is constructed using the function sample_linear_model from the previous problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_datasets(x,slope,intercept,sd,n):\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(sample_linear_model(x,slope,intercept,sd))\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_estimated_slope(x_vals,a=1,b=1,sd=1):\n",
    "    y = sample_datasets(x_vals,a,b,sd,1000)\n",
    "    slope_sum = 0\n",
    "    for i in range(1000):\n",
    "        slope_sum = slope_sum + compute_slope_estimator(x_vals,y[i,:])\n",
    "    return 1/1000*slope_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-1cfb2175b933>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-1cfb2175b933>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    In problem 7, I found that the value of average estimated slope that is returned is around 1 which is setted by us. As n increases, the value of average estimated slope is essentially unchanged.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "# n = 100\n",
    "# n = 1000\n",
    "x_vals = np.linspace(0,1,num=n)\n",
    "compute_average_estimated_slope(x_vals,a=1,b=1,sd=1)\n",
    "In problem 7, I found that the value of average estimated slope that is returned is around 1 which is setted by us. As n increases, the value of average estimated slope is essentially unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_estimated_slope_error(x_vals,a=1,b=1,sd=1):\n",
    "    y = sample_datasets(x_vals,a,b,sd,1000)\n",
    "    print(y[999,:])\n",
    "    error_sum = 0\n",
    "    for i in range(1000):\n",
    "        error_sum = error_sum + (1 - compute_slope_estimator(x_vals,y[i,:]))*(1 - compute_slope_estimator(x_vals,y[i,:]))\n",
    "    return 1/1000*error_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9977489156545863\n",
      "[0.80697344 1.18363036 0.73488395 ... 1.89262219 1.55918865 1.77579439]\n",
      "0.0011880378351885677\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "x_vals = np.linspace(0,1,num=m)\n",
    "print(compute_average_estimated_slope(x_vals,a=1,b=1,sd=1))\n",
    "print(compute_estimated_slope_error(x_vals,a=1,b=1,sd=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problem 8, I found that as n increases the average squared error decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 9. Sample 1000 training sets as in the previous problems, and calculate the estimated value of the slope on each of the 1000 training sets. Collect these 1000 samples together in a NumPy array. Using Matplotlib, create a histogram of these samples.\n",
    "Try this for different values of x_vals, as in Problems 7 and 8. What do you notice about these distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_1000_estimated_slope(x_vals,a=1,b=1,sd=1):\n",
    "    y = sample_datasets(x_vals,a,b,sd,1000)\n",
    "    slope = []\n",
    "    for i in range(1000):\n",
    "        slope.append(compute_slope_estimator(x_vals,y[i,:]))\n",
    "    return np.asarray(slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 1000\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x_vals = np.linspace(0,1,num=m)\n",
    "y_vals = compute_1000_estimated_slope(x_vals,a=1,b=1,sd=1)\n",
    "\n",
    "num_bins = 'auto'\n",
    "\n",
    "\n",
    "n, bins, patches = plt.hist(y_vals, num_bins , density=False, facecolor='g', alpha=0.75)\n",
    "# plt.xlim([0,2])\n",
    "plt.title('n = 1000')\n",
    "plt.savefig('n=1000.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problem 9, we can notice that the distributions for different values of x are all normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_error(y,y_hat):\n",
    "    n = y.shape[0]\n",
    "    sum_error = 0\n",
    "    for i in range(n):\n",
    "        sum_error = sum_error + (y[i]-y_hat[i])*(y[i]-y_hat[i])\n",
    "    return 1/n*sum_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_training_set_error(x_vals,a=1,b=1,sd=1):\n",
    "    y = sample_datasets(x_vals,a,b,sd,1000)\n",
    "    y_hat = np.zeros((1000,))\n",
    "    error = 0\n",
    "    for i in range(1000):\n",
    "        slope = compute_slope_estimator(x_vals,y[i,:])\n",
    "        intercept = compute_intercept_estimator(x_vals,y[i,:])\n",
    "        y_hat = slope *x_vals + intercept\n",
    "        error = error + calculate_prediction_error(y[i,:],y_hat)\n",
    "    return 1/1000*error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9768645158298158\n"
     ]
    }
   ],
   "source": [
    "m = 100\n",
    "x_vals = np.linspace(0,1,num=m)\n",
    "print(average_training_set_error(x_vals,a=1,b=1,sd=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When n = 5, 100, 1000, the value of error is respectively about 0.605,0.976,0.999. As the number of elements in x_vals increases, the average prediction error increases, and approaches 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_test_set_error(x_vals,a=1,b=1,sd=1):\n",
    "#     y_test = sample_linear_model(x_vals,a,b,sd)\n",
    "    y_train = sample_datasets(x_vals,a,b,sd,1000)\n",
    "    y_hat = np.zeros((1000,))\n",
    "    error = 0\n",
    "    for i in range(1000):\n",
    "        y_test = sample_linear_model(x_vals,a,b,sd)\n",
    "        slope = compute_slope_estimator(x_vals,y_train[i,:])\n",
    "        intercept = compute_intercept_estimator(x_vals,y_train[i,:])\n",
    "        y_hat = slope *x_vals + intercept\n",
    "        error = error + calculate_prediction_error(y_test,y_hat)\n",
    "    return 1/1000*error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997246186354058\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "x_vals = np.linspace(0,1,num=m)\n",
    "print(average_test_set_error(x_vals,a=1,b=1,sd=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problem 12, when n = 5, 100, 1000, the value of test set prediction error is 1.39, 1.024, 0.999. As the number of elements in x_vals increases, the average value of the test prediction error decreases, the average values of the training set prediction error increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-17-b032ea5e5a93>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-b032ea5e5a93>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    def compute_estimated_slope_error(x_vals,a=1,b=1,sd=1):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Problem 6\n",
    "def compute_average_estimated_slope(x_vals,a=1,b=1,sd=1):\n",
    "\n",
    "# Problem 7: Free response answer here.\n",
    "\n",
    "# Problem 8\n",
    "# Problem 8: Free response answer here.\n",
    "def compute_estimated_slope_error(x_vals,a=1,b=1,sd=1):\n",
    "\n",
    "# Problem 9: Include a pyplot graph as an additional file.\n",
    "\n",
    "# Problem 10\n",
    "def calculate_prediction_error(y,y_hat):\n",
    "\n",
    "# Problem 11\n",
    "# Problem 11: Free response answer here.\n",
    "def average_training_set_error(x_vals,a=1,b=1,sd=1):\n",
    "\n",
    "# Problem 12\n",
    "# Problem 12: Free response answer here.\n",
    "def average_test_set_error(x_vals,a=1,b=1,sd=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
